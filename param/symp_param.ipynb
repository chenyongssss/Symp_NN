{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6fc69c1a-6cb5-4f3e-b59c-053b87a3f0cd",
   "metadata": {},
   "source": [
    "## this is the second example: param linear wave equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b95f936-54e1-4f51-94b0-4fb215abe00d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# periodic bc wave equations\n",
    "import tensorflow as tf\n",
    "#import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import linalg as LA\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "import sys\n",
    "import time\n",
    "import os\n",
    "import h5py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b421873-2b52-4691-8cdb-d77194f563dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40acd316-f118-4f00-8229-ef42d845fd63",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tf.keras.utils.set_random_seed(1) \n",
    "tf.config.experimental.enable_op_determinism()\n",
    "\n",
    "\n",
    "dtype_=tf.float64\n",
    "tf.keras.backend.set_floatx('float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6961fd18-1adb-478f-b35b-660c775ee9c0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#If there are GPUs, the first one is used by default\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "print(\"Available GPUs:\",gpus)\n",
    "tf.config.set_visible_devices(gpus[0], 'GPU')\n",
    "import os\n",
    "os.environ['TF_FORCE_GPU_ALLOW_GROWTH']=\"true\"   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df2692e7-99ad-445e-bb41-fadad8dc88f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_path = os.getcwd()   # the current directory\n",
    "print(curr_path)\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecad5b6d-c975-4ec4-bf32-df226b3ac01a",
   "metadata": {},
   "source": [
    "### building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8737fff4-c8a9-442c-b96e-9ff58a963ef0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# the optional block of our symplectic NN, G-reflector\n",
    "class SympMatLayer(layers.Layer):\n",
    "    def __init__(self, nif):\n",
    "        super(SympMatLayer, self).__init__()\n",
    "        init = tf.initializers.GlorotNormal(seed=0)\n",
    "        #init_zero = tf.zeros_initializer()\n",
    "\n",
    "        self.nif = nif\n",
    "        # beta_init = init(shape=[1,1], dtype = dtype_)\n",
    "        # uvec_init = init(shape=[2*self.dim,1], dtype = dtype_)\n",
    "\n",
    "        self.beta = self.add_weight(name = 'beta', shape=[1,1], initializer=init, trainable=True)\n",
    "\n",
    "        self.uvec = self.add_weight(name = 'u', shape=[2*self.nif,1], initializer=init, trainable=True)\n",
    "\n",
    "\n",
    "    @tf.function\n",
    "    def call(self,z):\n",
    "        zJt = tf.concat([z[:,self.nif:],-z[:,0:self.nif]], axis=1)\n",
    "        out = z + self.beta*tf.matmul(tf.matmul(zJt, self.uvec), tf.transpose(self.uvec))\n",
    "        return out\n",
    "\n",
    "    '''\n",
    "        Note the inverse of G is\n",
    "        G^{-1} = I - beta u u^T J\n",
    "        Comment: this is a nice property!\n",
    "    '''\n",
    "    @tf.function\n",
    "    def inverse(self, z):\n",
    "        zJt = tf.concat([z[:,self.nif:],-z[:,0:self.nif]], axis=1)\n",
    "        out = z - self.beta*tf.matmul(tf.matmul(zJt, self.uvec), tf.transpose(self.uvec))\n",
    "        return out\n",
    "    \n",
    "\n",
    "# great symp matrix with G-reflectors\n",
    "\n",
    "class SympMat(Model):\n",
    "    \"\"\"\n",
    "    N: number of G-reflectors\n",
    "    nif: dim of full physical space\n",
    "    nir: dim of reduced latent space\n",
    "    \"\"\"\n",
    "    def __init__(self,N,nif,nir):\n",
    "        super(SympMat, self).__init__()\n",
    "        self.N = N\n",
    "        self.nif = nif\n",
    "        self.nir = nir\n",
    "        self.hlayers=[]\n",
    "        for i in range(self.N):\n",
    "            hl = SympMatLayer(self.nif)\n",
    "            self.hlayers.append(hl)\n",
    "\n",
    "\n",
    "        self.paddings = tf.constant([[0,0], [0,self.nif-nir]])\n",
    "\n",
    "\n",
    "    @tf.function\n",
    "    def project(self, r):\n",
    "        rout = r\n",
    "        for i in range(self.N):\n",
    "            rout = self.hlayers[i](rout)\n",
    "\n",
    "        return tf.concat([rout[:,:self.nir],rout[:,self.nif:(self.nif+self.nir)]], axis=1)\n",
    "\n",
    "    @tf.function\n",
    "    def inverse(self, r):\n",
    "        # K = r.shape[0]\n",
    "        # d = self.nif-self.nir\n",
    "\n",
    "        q = r[:,:self.nir]\n",
    "        q = tf.pad(q, self.paddings, \"CONSTANT\")\n",
    "        p = r[:,self.nir:]\n",
    "        p = tf.pad(p, self.paddings, \"CONSTANT\")\n",
    "\n",
    "        rout = tf.concat([q,p], axis=1)\n",
    "        for i in range(self.N)[-1::-1]:\n",
    "            rout = self.hlayers[i].inverse(rout)\n",
    "\n",
    "        return rout\n",
    "\n",
    "\n",
    "    @tf.function\n",
    "    def call(self, r):\n",
    "        rout = r\n",
    "\n",
    "        # mask = np.ones_like(r.numpy())\n",
    "        # mask[:,self.nir:self.nif] = 0.\n",
    "        # mask[:,(self.nif+self.nir):] = 0.\n",
    "\n",
    "        # mask = tf.constant(mask, dtype=tf.float64)\n",
    "\n",
    "        for i in range(self.N):\n",
    "            rout = self.hlayers[i](rout)\n",
    "\n",
    "        # print( tf.concat([rout[:,:self.nir], tf.zeros_like(rout)[:,:(self.nif-self.nir)],rout[:,self.nif:(self.nif+self.nir)]], axis=1))\n",
    "        # rout = rout*mask\n",
    "\n",
    "        rout = tf.concat([rout[:,:self.nir], tf.zeros_like(rout)[:,:(self.nif-self.nir)],\n",
    "                          rout[:,self.nif:(self.nif+self.nir)],\n",
    "                          tf.zeros_like(rout)[:,:(self.nif-self.nir)]\n",
    "                          ], axis=1)\n",
    "\n",
    "        for i in range(self.N)[-1::-1]:\n",
    "            rout = self.hlayers[i].inverse(rout)\n",
    "\n",
    "\n",
    "\n",
    "        return rout\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0af2f31-4f31-4c69-a8d1-8287f2ee5fa1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# the main block of our symplectic NN, HenonNets\n",
    "#potential function\n",
    "class Potential(layers.Layer):\n",
    "    def __init__(self, units=5):\n",
    "        super(Potential, self).__init__()\n",
    "        self.units = units\n",
    "        self.dense_1 = Dense(name='potential.1', units = self.units, activation='elu') # layer class Linear has to be defined before\n",
    "        self.dense_2 = Dense(name='potential.2', units = self.units, activation='elu')\n",
    "        self.out = Dense(1,use_bias=False) # output size is 1, no bias\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.dense_1(inputs)\n",
    "        x = self.dense_2(x)\n",
    "        return self.out(x)\n",
    "\n",
    "\n",
    "ymean_tf = tf.constant(0, dtype=tf.float64)\n",
    "ydiam_tf = tf.constant(1., dtype=tf.float64)\n",
    "\n",
    "\n",
    "class HenonLayer(layers.Layer):\n",
    "    def __init__(self, units=10):\n",
    "        super(HenonLayer, self).__init__()\n",
    "        self.units = units\n",
    "        self.V = Potential(self.units)\n",
    "\n",
    "    def build(self, input_shape): # the input_shape is the shape of z in the call function\n",
    "        self.eta = self.add_weight(name = 'eta', shape=(input_shape[-1]//2,), trainable=True)\n",
    "\n",
    "    def call(self, z, inverse = False):\n",
    "        if inverse:\n",
    "            return self.inverse(z)\n",
    "        return self.forward(z)\n",
    "\n",
    "    @tf.function\n",
    "    def forward(self, z):\n",
    "        #ipdb.set_trace()\n",
    "        ni = int(z.shape[-1]/2)\n",
    "        X = z[:,:ni]\n",
    "        Y = z[:,ni:2*ni]\n",
    "        kw = z[:,-1:]\n",
    "        for k in range(4):\n",
    "            with tf.GradientTape() as tape:\n",
    "                tape.watch(Y)\n",
    "                Ylast = (tf.concat([Y,kw],axis=1) - ymean_tf) / ydiam_tf\n",
    "                p = self.V(Ylast)\n",
    "\n",
    "            Yout = -X + tape.gradient(p, Y)\n",
    "            X = Y + self.eta\n",
    "            Y = Yout\n",
    "\n",
    "        return tf.concat([X,Y,kw], axis = 1)#batch*(2n+1)\n",
    "    @tf.function\n",
    "    def inverse(self, z): # compute the inverse of the HenonLayer\n",
    "\n",
    "        ni = int(z.shape[-1]/2)\n",
    "        X = z[:,:ni]\n",
    "        Y = z[:,ni:2*ni]\n",
    "        kw = z[:,-1:]\n",
    "        for k in range(4):\n",
    "            X = X - self.eta\n",
    "            with tf.GradientTape() as tape:\n",
    "                tape.watch(X)\n",
    "                Xlast = (X - ymean_tf)/ydiam_tf\n",
    "                p = self.V(tf.concat([Xlast,kw],axis=1))\n",
    "            Ylast = X\n",
    "            X = -Y + tape.gradient(p, X)\n",
    "            Y = Ylast\n",
    "\n",
    "        return tf.concat([X,Y,kw], axis = 1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6940343-106e-46d9-9256-3b4af92b6d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SympMatROM(Model):\n",
    "    \"\"\"\n",
    "        units_list: the list of\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, units_list, N, nif,nir):\n",
    "        super(SympMatROM, self).__init__()\n",
    "        self.loss_tracker = tf.keras.metrics.Mean(name=\"loss\")\n",
    "        self.N = N\n",
    "        self.nif = nif\n",
    "        self.nir = nir\n",
    "        self.matlayers = []\n",
    "        self.henonlayers = []\n",
    "\n",
    "        for i in units_list:\n",
    "            hl = HenonLayer(i)\n",
    "            self.henonlayers.append(hl)\n",
    "\n",
    "        for i in range(self.N):\n",
    "            ml = SympMatLayer(self.nif)\n",
    "            self.matlayers.append(ml)\n",
    "\n",
    "\n",
    "        self.paddings = tf.constant([[0,0], [0,self.nif-self.nir]])\n",
    "\n",
    "\n",
    "    @tf.function\n",
    "    def project(self, r):\n",
    "        rout = r\n",
    "        for i in range(len(self.henonlayers)):\n",
    "            rout = self.henonlayers[i](rout)\n",
    "        for i in range(self.N):\n",
    "            rout = self.matlayers[i](rout)\n",
    "\n",
    "        return tf.concat([rout[:,:self.nir],rout[:,self.nif:(self.nif+self.nir)]], axis=1)\n",
    "\n",
    "    @tf.function\n",
    "    def embedding(self, r):\n",
    "        # K = r.shape[0]\n",
    "        # d = self.nif-self.nir\n",
    "\n",
    "        q = r[:,:self.nir]\n",
    "        q = tf.pad(q, self.paddings, \"CONSTANT\")\n",
    "        p = r[:,self.nir:]\n",
    "        p = tf.pad(p, self.paddings, \"CONSTANT\")\n",
    "\n",
    "        rout = tf.concat([q,p], axis=1)\n",
    "        for i in range(self.N)[-1::-1]:\n",
    "            rout = self.matlayers[i].inverse(rout)\n",
    "\n",
    "        for i in range(len(self.henonlayers))[-1::-1]:\n",
    "            rout = self.henonlayers[i].inverse(rout)\n",
    "\n",
    "        return rout\n",
    "\n",
    "\n",
    "    @tf.function\n",
    "    def call(self, r):\n",
    "        rout = r\n",
    "\n",
    "\n",
    "        rout = self.project(rout)\n",
    "        rout = self.embedding(rout)\n",
    "\n",
    "\n",
    "\n",
    "        return rout\n",
    "\n",
    "    def train_step(self, data):\n",
    "        # Unpack the data. Its structure depends on your model and\n",
    "        # on what you pass to `fit()`.\n",
    "        x, y = data\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = self(x, training=True)  # Forward pass\n",
    "            # Compute the loss value\n",
    "           \n",
    "            loss = self.compute_loss(y=y, y_pred=y_pred)\n",
    "\n",
    "        # Compute gradients\n",
    "        trainable_vars = self.trainable_variables\n",
    "        gradients = tape.gradient(loss, trainable_vars)\n",
    "        # Update weights\n",
    "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "        # Update metrics (includes the metric that tracks the loss)\n",
    "        # need to update the loss in loss_tracker\n",
    "        self.loss_tracker.update_state(loss) # Add current batch loss\n",
    "\n",
    "        # Return a dict mapping metric names to current value\n",
    "        return {\"loss\": self.loss_tracker.result()}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def test_step(self, data):\n",
    "        # Unpack the data. Its structure depends on your model and\n",
    "        # on what you pass to `fit()`.\n",
    "        x, y = data\n",
    "\n",
    "\n",
    "        y_pred = self(x, training=False)  # Forward pass\n",
    "        loss = self.compute_loss(y=y, y_pred=y_pred)\n",
    "        self.loss_tracker.update_state(loss) # Add current batch loss\n",
    "\n",
    "\n",
    "        # Return a dict mapping metric names to current value\n",
    "        return {\"loss\": self.loss_tracker.result()}\n",
    "        # return {\"mae\": self.mae_metric.result()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44cc29bc-752c-448a-b588-8844e4accc0c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def hamiltonian(z):\n",
    "  \"\"\"\n",
    "  compute the discrete hamiltonian, it depends on the concrete equation, and the physical space; aiming at computing the hamiltonian loss\n",
    "  \"\"\"\n",
    "  kw = z[:,-1:] # wave speed\n",
    "  delta_x = 1/200 # grid spacing\n",
    "  q = z[:,:z.shape[-1]//2]# q\n",
    "  p = z[:,z.shape[-1]//2:z.shape[-1]//2*2]# p\n",
    "  q_r = tf.concat([q[:,1:],q[:,0:1]],axis=1)# q1-qN+1\n",
    "  q_l = tf.concat([q[:,-1:],q[:,:-1]],axis=1)# q0-qN-1\n",
    "\n",
    "  h = delta_x/2*tf.reduce_sum(p**2+kw*(q_r-q)**2/(2*delta_x**2)+kw*(q-q_l)**2/(2*delta_x**2),keepdims=True,axis=1)\n",
    "  return h\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6658e447-aa9b-4c01-b301-897bede4a9ec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SympMatFlowMappingROM(Model):\n",
    "    \"\"\"\n",
    "        units_list: the list of \n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, units_list_ae, N, nif, nir, units_list_fm, n, n_train_time,lamb1):\n",
    "        super(SympMatFlowMappingROM, self).__init__()\n",
    "        self.loss_tracker = tf.keras.metrics.Mean(name=\"loss\")\n",
    "        self.N = N\n",
    "        self.nif = nif\n",
    "        self.nir = nir\n",
    "        self.matlayers_ae = []\n",
    "        self.henonlayers_ae = []\n",
    "        self.n = n\n",
    "        self.matlayers_fm = []\n",
    "        self.henonlayers_fm = []\n",
    "        self.n_train_time = n_train_time\n",
    "        self.lamb1 = lamb1 # balanced hamilton loss\n",
    "\n",
    "        for i in units_list_ae:\n",
    "            hl = HenonLayer(i)\n",
    "            self.henonlayers_ae.append(hl)\n",
    "\n",
    "        for i in range(self.N):\n",
    "            ml = SympMatLayer(self.nif)\n",
    "            self.matlayers_ae.append(ml)\n",
    "\n",
    "        for i in units_list_fm:\n",
    "            hl = HenonLayer(i)\n",
    "            self.henonlayers_fm.append(hl)\n",
    "\n",
    "        for i in range(self.n):\n",
    "            ml = SympMatLayer(self.nir)\n",
    "            self.matlayers_fm.append(ml)\n",
    "\n",
    "        self.paddings = tf.constant([[0,0], [0,self.nif-self.nir]])\n",
    "\n",
    "     \n",
    "    @tf.function\n",
    "    def project(self, r):\n",
    "        rout = r# forward  batch*(2n+1)\n",
    "        for i in range(len(self.henonlayers_ae)):\n",
    "            rout = self.henonlayers_ae[i](rout) #batch*(2n+1)\n",
    "        rout1 = rout[:,:2*self.nif]\n",
    "        for i in range(self.N):\n",
    "            rout1 = self.matlayers_ae[i](rout1)#batch*2n\n",
    "\n",
    "        return tf.concat([rout1[:,:self.nir],rout1[:,self.nif:(self.nif+self.nir)],rout[:,-1:]], axis=1) #batch*(2nir+1),the last dim is kw\n",
    "            \n",
    "    @tf.function\n",
    "    def embedding(self, r):\n",
    "        # K = r.shape[0]\n",
    "        # d = self.nif-self.nir\n",
    "\n",
    "        q = r[:,:self.nir]\n",
    "        q = tf.pad(q, self.paddings, \"CONSTANT\")\n",
    "        p = r[:,self.nir:2*self.nir]\n",
    "        p = tf.pad(p, self.paddings, \"CONSTANT\")\n",
    "        kw = r[:,-1:]\n",
    "        rout = tf.concat([q,p], axis=1)\n",
    "        for i in range(self.N)[-1::-1]:\n",
    "            rout = self.matlayers_ae[i].inverse(rout)\n",
    "        rout1 = tf.concat([rout,kw], axis=1)#batch*(2n+1)\n",
    "        for i in range(len(self.henonlayers_ae))[-1::-1]:\n",
    "            rout1 = self.henonlayers_ae[i].inverse(rout1)\n",
    "\n",
    "        return rout1\n",
    "\n",
    "    @tf.function\n",
    "    def flowmapping(self, r):\n",
    "\n",
    "        q = r[:,:self.nir]\n",
    "        p = r[:,self.nir:2*self.nir]\n",
    "        kw = r[:,-1:]\n",
    "        rout = tf.concat([q,p], axis=1)\n",
    "        for i in range(self.n):\n",
    "            rout = self.matlayers_fm[i](rout)\n",
    "        rout1 = tf.concat([rout,kw],axis=1)#2*nir+1\n",
    "        for i in range(len(self.henonlayers_fm)):\n",
    "            rout1 = self.henonlayers_fm[i](rout1)#2*nir+1\n",
    "\n",
    "        return rout1\n",
    "    \n",
    "    def inverse(self,r):\n",
    "        rout = r\n",
    "        for i in range(len(self.henonlayers_fm))[-1::-1]:\n",
    "            rout = self.henonlayers_fm[i].inverse(rout)#2*nir+1\n",
    "        rout1 = rout[:,:2*self.nir]\n",
    "        for i in range(self.n)[-1::-1]:\n",
    "            rout1 = self.matlayers_fm[i].inverse(rout1)\n",
    "        return tf.concat([rout1,rout[:,-1:]], axis=1)\n",
    "\n",
    "\n",
    "\n",
    "    @tf.function\n",
    "    def call(self, r):\n",
    "        rout = r\n",
    "\n",
    "        rout = self.project(rout)\n",
    "        rout = self.embedding(rout)\n",
    "        \n",
    "        return rout\n",
    "\n",
    "    def train_step(self, data):\n",
    "        # Unpack the data. Its structure depends on your model and\n",
    "        # on what you pass to `fit()`. #x batch*401\n",
    "        x1, y = data\n",
    "        x = x1[:,:self.nif*2]#batch*400\n",
    "        # kw = x1[:,-1:]#batch*1\n",
    "\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            z = self.project(x1)#2nir+1\n",
    "            y0 = self.embedding(z)#2n+1\n",
    "            ham_cons = hamiltonian(x)\n",
    "            loss_enc = self.compute_loss(y=x, y_pred=y0[:,:-1])\n",
    "            loss_ham = self.compute_loss(y=ham_cons, y_pred = hamiltonian(y0))\n",
    "\n",
    "            for i in range(self.n_train_time):\n",
    "                z = self.flowmapping(z)  # Forward pass 2nir+1\n",
    "               \n",
    "                y0 = self.embedding(z) #2n+1\n",
    "                loss_enc += self.compute_loss(y=y[:,i,:], y_pred=y0[:,:-1])\n",
    "                loss_ham += self.compute_loss(y=ham_cons, y_pred = hamiltonian(y0))\n",
    "            loss = loss_enc + self.lamb1*loss_ham\n",
    "\n",
    "        # Compute gradients\n",
    "        trainable_vars = self.trainable_variables\n",
    "        gradients = tape.gradient(loss, trainable_vars)\n",
    "        # Update weights\n",
    "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "        # Update metrics (includes the metric that tracks the loss)\n",
    "        # need to update the loss in loss_tracker\n",
    "        self.loss_tracker.update_state(loss) # Add current batch loss\n",
    "\n",
    "        # Return a dict mapping metric names to current value\n",
    "        return {\"loss\": self.loss_tracker.result()}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f28001d3-6239-4f08-9531-621b88250fe8",
   "metadata": {},
   "source": [
    "### load data and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f6312c-d9d5-42a5-b76f-a1f178a3252a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# parameters\n",
    "c2 = 0.1\n",
    "w_all = np.linspace(0,1,5)\n",
    "k_w = np.zeros((5,5,5,5))\n",
    "for i in range(5):\n",
    "    for j in range(5):\n",
    "        for k in range(5):\n",
    "            for p in range(5):\n",
    "                k_w[i,j,k,p] = c2*(w_all[i]+w_all[j]/4+w_all[k]/9+w_all[p]/16)\n",
    "k_w = k_w.reshape(-1,1)\n",
    "k_w1 = np.repeat(k_w,repeats = 44, axis=1)\n",
    "k_w1 = k_w1.T\n",
    "k_w1 = k_w1.reshape(-1,1)\n",
    "k_w1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e78e8e7c-3355-46e8-8b0f-2451546665b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "nlayer_ae = 2\n",
    "nwidth_ae = 512\n",
    "\n",
    "nlayer_fm = 2\n",
    "nwidth_fm = 128\n",
    "\n",
    "example_time_steps = 6\n",
    "\n",
    "\n",
    "#activiation = 'relu'\n",
    "batch_size = 256\n",
    "epoch = 5000\n",
    "\n",
    "initial_learning_rate = 1e-3\n",
    "decay_steps=13 ## decay for every three epoch\n",
    "decay_rate=0.99\n",
    "\n",
    "n_x = 200 # data dimension\n",
    "n_sample = 625\n",
    "\n",
    "\n",
    "nif = n_x # dimension of full space\n",
    "nir = 10 # dimension of reduced space\n",
    "\n",
    "nref = nir # number of reflectors\n",
    "\n",
    "verbose = 2\n",
    "\n",
    "data_str = \"Wave_bump_peri\"\n",
    "\n",
    "data_features_str = f\"Wave_bump_peri_features_c_0.1_Nx_{n_x}_sample_{n_sample}_timesteps_44_dt_3.0\"\n",
    "data_labels_str = f\"Wave_bump_peri_labels_c_0.1_Nx_{n_x}_sample_{n_sample}_timesteps_44_dt_3.0\"\n",
    "\n",
    "para_str = data_str + f\"_nlayer_ae_{nlayer_ae}_hidden_ae_{nwidth_ae}\"\n",
    "para_str = para_str + f\"_nlayer_fm_{nlayer_fm}_hidden_fm_{nwidth_fm}\"\n",
    "para_str = para_str + f\"_nreflector_{nref}_lr_{initial_learning_rate}_decaystep_{decay_steps}\"\n",
    "para_str = para_str + f\"_decayrate_{decay_rate}_batch_{batch_size}_epoch_{epoch}\"\n",
    "\n",
    "\n",
    "data_features_path = os.path.join(curr_path, \"data\", data_features_str + \".npy\")\n",
    "data_labels_path = os.path.join(curr_path, \"data\", data_labels_str + \".npy\")\n",
    "\n",
    "train_path = os.path.join(curr_path, \"train\", para_str + \".log\")\n",
    "check_path = os.path.join(curr_path, \"check\", para_str)\n",
    "\n",
    "\n",
    "data_features = np.load(data_features_path)\n",
    "data_labels = np.load(data_labels_path)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e28f86b-7aca-431c-aa65-7c7cd0c1f35d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "features1 = np.concatenate([data_features,k_w1],axis=1)\n",
    "# features1.shape\n",
    "# here train for the autoencoder\n",
    "dataset = tf.data.Dataset.from_tensor_slices((features1, data_labels))\n",
    "dataset = dataset.shuffle(len(dataset)) # must include the shuffle size\n",
    "\n",
    "DATASET_SIZE = len(dataset)\n",
    "\n",
    "train_size = int(1.0 * DATASET_SIZE)\n",
    "\n",
    "train_dataset = dataset.take(train_size)\n",
    "test_dataset = dataset.skip(train_size)\n",
    "\n",
    "\n",
    "\n",
    "train_dataset = train_dataset.shuffle(len(train_dataset))\n",
    "train_dataset = train_dataset.batch(batch_size)\n",
    "\n",
    "test_dataset = test_dataset.batch(batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3759d95c-4b07-41a0-8a2b-87f3a0494bc7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# balance the reconstruction loss and hamiltonian loss\n",
    "lamb1 = 0.01\n",
    "if lamb1!=0:\n",
    "    para_str = para_str + f\"_lamb1_{lamb1}\"\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b355e1-4235-4caa-9212-507695b1930d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "directory = os.path.dirname(train_path)\n",
    "\n",
    "# if not exist, create the directory\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "with open(train_path, 'a') as f:\n",
    "\n",
    "    f.write('log_file_name:  {:s} \\n'.format(train_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f37d5f62-dd3f-41b3-ac88-8cd7b5346d87",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "loss_fun = keras.losses.MeanSquaredError()\n",
    "test_model = SympMatFlowMappingROM([nwidth_ae]*nlayer_ae,nref,nif,nir,[nwidth_fm]*nlayer_fm,nref,example_time_steps,lamb1)\n",
    "# test_model = SympMatFlowMappingROM([nwidth_ae]*nlayer_ae,nir,nif,nir,[nwidth_fm]*nlayer_fm,nir,example_time_steps)\n",
    "\n",
    "reload = False # reload the weights, If it is the first time training, it needs to be set to false\n",
    "train = True # train or test the model\n",
    "\n",
    "\n",
    "batch_number = train_size//batch_size\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate,\n",
    "    # 1e-4,\n",
    "    decay_steps=batch_number*decay_steps,\n",
    "    decay_rate=decay_rate,\n",
    "    staircase=True)\n",
    "Adamoptimizer = tf.keras.optimizers.Adam(learning_rate = lr_schedule)\n",
    "# Train from a checkpt vs start a new training\n",
    "checkpoint_path = os.path.join(curr_path, \"check\", para_str,\"cp-{epoch:05d}.ckpt\")\n",
    "if reload:\n",
    "    checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "    latest = tf.train.latest_checkpoint(checkpoint_dir)\n",
    "    print('Load the previous training. The old model is in:')\n",
    "    print(latest)\n",
    "    epochs=latest.replace(check_path+'/', '')\n",
    "    epochs=epochs.replace('cp-', '')\n",
    "    epochs=epochs.replace('.ckpt', '')\n",
    "    epoch0=4080\n",
    "    print(\"Epoch0 = %5d\" % epoch0)\n",
    "    test_model.load_weights(latest)\n",
    "else:\n",
    "    epoch0=0\n",
    "    print('start training from the begining...')\n",
    "test_model.compile(optimizer = Adamoptimizer, loss = loss_fun)\n",
    "\n",
    "# Create a callback that saves the model's weights every 10 epochs\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path, verbose=verbose, save_weights_only=True, period=40)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf1f707-8bbf-4f8a-9b5e-22ce998980ec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "# if not exist, create the file\n",
    "if not os.path.exists(checkpoint_dir):\n",
    "    os.makedirs(checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebdad365-68d4-40a0-bb7b-f7402beca887",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LearningRatePrintingCallback(keras.callbacks.Callback):\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        tf.print(\n",
    "           'lr: ',test_model.optimizer.lr\n",
    "            )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if train:\n",
    "    # Train the model\n",
    "    print('training model...')\n",
    "    h = test_model.fit(train_dataset, epochs = epoch,\n",
    "                    #    validation_data=test_dataset,\n",
    "                       verbose=verbose, initial_epoch=epoch0,\n",
    "                       callbacks = [cp_callback,LearningRatePrintingCallback()]) #\n",
    "    print('training complete...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "730ef2f8-73df-4448-92a0-9de6f5e901ae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#test the reconstruction ability of the autoencoder\n",
    "data_rec = test_model(features1)\n",
    "\n",
    "rec_error = tf.reduce_mean(tf.square(features1[:,:-1]-data_rec[:,:-1]))\n",
    "print(rec_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af55d9fb-0f45-491d-9a2a-6589b9d1e0b2",
   "metadata": {},
   "source": [
    "### test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "620e12aa-e038-493d-98fc-a7acb390d27f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_features1 = features1.reshape(44,625,401)\n",
    "data_labels1 = data_labels.reshape(44,625,6,400)[-1].transpose(1,0,2)\n",
    "k_w2 = np.stack([k_w]*6,axis=0)\n",
    "data_labels1 = np.concatenate([data_labels1,k_w2],axis=2)\n",
    "data_features_plus = np.concatenate([data_features1,data_labels1],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac11b3a4-8ba3-4526-92bc-5fc2eb7e95cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_features_low = test_model.project(data_features_plus.reshape(-1,401))\n",
    "data_features_low.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd4e66b2-df05-46f6-8691-df9009f217de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_low = data_features_low.numpy().reshape(50,625,21)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac44472a-b7ba-499e-8dbd-a50717a3662c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#latent dynamics\n",
    "times = np.linspace(0,3,50)\n",
    "plt.figure(figsize=(8,6))\n",
    "for i in range(10):\n",
    "    plt.plot(times,data_low[:,24,i])\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Magnitude')\n",
    "plt.title('Latent Space - 1D linear wave')\n",
    "# plt.plot(times,x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3804b19-4e98-4fec-9969-6aaf762d8828",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#  time evolution, here you can choose the final time(delta t* time_step), for this example, delta t is chosen as 0.06\n",
    "time_step = 49\n",
    "ham_cons = []\n",
    "x_test = data_features_plus[0,:,:]#625*401\n",
    "ham_cons.append(hamiltonian(x_test))\n",
    "z_test = test_model.project(x_test)#625*21\n",
    "for i in range(time_step):\n",
    "    \n",
    "    z_test = test_model.flowmapping(z_test)#625*21\n",
    "    \n",
    "    y_test = test_model.embedding(z_test)#625*401\n",
    "    ham_cons.append(hamiltonian(y_test))\n",
    "y_test = test_model.embedding(z_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "512bf5b5-140d-4db2-956e-c58849e3cee5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ham_cons = tf.concat(ham_cons,axis=1)\n",
    "ham_cons.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c4ef52-b154-4090-86a1-a3fa565c6057",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#choose one trajectory and plot the trajectory, \n",
    "index = 24\n",
    "plt.figure(figsize=(8,6))\n",
    "x = np.linspace(0, 1, n_x)\n",
    "# sparse plot\n",
    "sparse_step = 5  \n",
    "# plt.plot(x, y_test[24, :n_x], c='g', marker='o', markersize=6, label='Sym', markevery=sparse_step)\n",
    "plt.plot(x, y_test[index, :n_x], c='g', marker='o', markersize=6, label='Sym', markevery=sparse_step)\n",
    "plt.plot(x, data_features_plus[49, index, :n_x], c='b', linestyle='-', label='Exact')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('Magnitude')\n",
    "plt.legend(loc='best')\n",
    "# plt.title('Relative Error:')\n",
    "plt.title('Recovered solution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d83a7230-5b4c-434a-baef-2e919a0484b0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#choose one trajectory, show the evolution of hamiltonian \n",
    "\n",
    "index = 100\n",
    "ham_label = hamiltonian(data_features_plus[:,index])\n",
    "\n",
    "times = np.linspace(0, 3, 50)\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(times,ham_cons[index],c='g', marker='o', markersize=6, label='Sym')\n",
    "plt.plot(times,ham_label,c='b', linestyle='-', label='Exact')\n",
    "# plt.plot(times,ham_cons[2,1:],c='y',label='Sample = 2')\n",
    "plt.xlabel('Time')\n",
    "# plt.ylabel('Magnitude')\n",
    "plt.legend(loc='best')\n",
    "plt.ylim(0,0.4)\n",
    "# plt.title('Relative Error:')\n",
    "plt.title('Hamiltonian')\n",
    "plt.ylabel('H(t)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "012e986d-24a2-41dd-848a-db5c17108f1c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow-2.15.0",
   "language": "python",
   "name": "tensorflow-2.15.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
